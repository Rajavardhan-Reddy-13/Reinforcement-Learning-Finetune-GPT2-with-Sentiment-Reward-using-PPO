# ğŸ¤– GPT-2 Fine-Tuning with RLHF and Sentiment-Based Rewards (PPO + IMDB)

This project fine-tunes a **GPT-2 language model** using **Reinforcement Learning from Human Feedback (RLHF)** to generate **more positive and human-aligned text responses**. The model is trained on movie review prompts and rewarded based on the **positivity of its responses**, as judged by a sentiment analysis model. The training uses the **Proximal Policy Optimization (PPO)** algorithm and is inspired by real-world RLHF techniques used to align large language models with human preferences.

---

## ğŸ“Œ Project Goal

To train GPT-2 to generate **positive movie review responses** by using **reward scores** from a sentiment classifier and optimizing behavior using **reinforcement learning (PPO)**.

---

## ğŸ“š Dataset

### ğŸ”¹ IMDB Movie Reviews
- Source: Hugging Face Datasets
- Description: Contains 50,000+ movie reviews labeled as positive or negative.
- Use: Prompts are extracted from long reviews to train the model on realistic user inputs.

---

## ğŸ§  Models Used

- **GPT-2 (`lvwerra/gpt2-imdb`)**  
  > Base language model used for response generation and PPO optimization.

- **DistilBERT Sentiment Classifier (`lvwerra/distilbert-imdb`)**  
  > Reward model that scores the positivity of generated responses.

---

## ğŸ› ï¸ Tools & Libraries

- **Python**
- **Google Colab** (training environment)
- **Hugging Face Libraries:**
  - `transformers` â€“ model and tokenizer handling
  - `datasets` â€“ loading IMDB data
  - `trl` â€“ PPO-based RLHF training
- **Weights & Biases (wandb)** â€“ experiment logging
- **PyTorch** â€“ deep learning backend

---

## ğŸš€ Implementation Steps

### âœ… 1. Install Required Libraries
```bash
pip install transformers==4.37.2
pip install datasets==2.16.1
pip install trl==0.7.10
pip install tqdm==4.66.1
pip install torch==2.2.0
pip install peft==0.10.0
pip install "numpy<2"
```

### âœ… 2. Load Dataset and Tokenizer
- Load IMDB dataset and extract only long reviews.
- Use short excerpts (2â€“8 tokens) from reviews as **prompts**.

### âœ… 3. Initialize Models
- GPT-2 with a **value head** is used for PPO training.
- A **reference GPT-2** is used to measure divergence.
- Tokenizer pad token is set to EOS for consistency.

### âœ… 4. Sentiment-Based Reward Function
- A **DistilBERT sentiment classifier** is used to assign rewards:
  - Higher reward for more **positive** responses.
  - Only the **"POSITIVE" score** is used in PPO training.

### âœ… 5. Training Loop with PPO
- For each prompt:
  - GPT-2 generates a response.
  - Prompt + response is sent to the reward model.
  - PPOTrainer uses the reward to improve GPT-2's behavior.
- PPO minimizes deviation from the reference model while maximizing reward.

### âœ… 6. Evaluation
- A batch of prompts is run through:
  - Original GPT-2 (before training)
  - Fine-tuned GPT-2 (after training)
- Both outputs are scored using the sentiment classifier.
- Comparison is printed using **mean** and **median** scores.

---

## âœ… Example Output

**Prompt:**
```
This movie had amazing acting and a beautiful story.
```

**Before Fine-Tuning:**
```
But the ending was boring and predictable.
```

**After Fine-Tuning (RLHF + PPO):**
```
Itâ€™s one of the most heartwarming films Iâ€™ve seen!
```

âœ… Shows how the model learns to favor **positive, helpful responses** over negative ones.

---

## ğŸ“Š Results Summary

| Metric                   | Before Training | After Training |
|--------------------------|------------------|-----------------|
| Avg. Sentiment Score     | ~0.34            | ~0.81           |
| Output Tone              | Mixed/Neutral    | Positive         |
| Alignment with Human Preference | Low         | High             |

---

## ğŸ“ˆ Future Enhancements

- Train for more PPO steps with longer prompts
- Add constraints for fluency or coherence (using custom rewards)
- Evaluate using human preference labels (if available)
- Integrate outputs into chat-style interface using Gradio

---

## ğŸ§¾ References

- ğŸ”— TRL PPO Example: [https://github.com/huggingface/trl](https://github.com/huggingface/trl)
- ğŸ”— PPO Paper (OpenAI): [https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)
- ğŸ”— YouTube Guide: [https://www.youtube.com/watch?v=qGyFrqc34yc](https://www.youtube.com/watch?v=qGyFrqc34yc)
- ğŸ”— IMDB Dataset: [https://huggingface.co/datasets/imdb](https://huggingface.co/datasets/imdb)
- ğŸ”— Starter Code Repo: [https://github.com/hkproj/rlhf-ppo](https://github.com/hkproj/rlhf-ppo)
- ğŸ”— Hugging Face GPT-2 Sentiment Example: [https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb](https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)

---

## Â© Copyright

```
This project is for academic and research use only.  
All datasets and models used are subject to their respective licenses.
```
